# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d0n2UBrFoYnda8vEWkpN_cwaC1obotNF
"""

"""
Module: Statutes_at_5.py
------------------------
Scrapes, parses, and loads all Florida statutes into a SQLite database.
- Robust, configurable, resumable, and well-tested.
- See README.md for usage and configuration details.
"""

# Place all core scraping functions at the top level for import and testing
import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import urljoin, urlparse, parse_qs
import sqlite3
import logging
import configparser

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')
logger = logging.getLogger(__name__)

def fetch_html(url, retries=3, delay=1):
    """
    Fetch HTML content from a URL with retries and delay.
    Returns (BeautifulSoup object, raw content) or (None, None) on failure.
    """
    for i in range(retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser'), response.content
        except requests.exceptions.RequestException:
            if i < retries - 1:
                time.sleep(delay)
            else:
                return None, None

def get_title_links(soup, required_titles):
    """
    Extracts title links from the statutes index page soup for the required titles.
    Returns a list of dicts: {text, url}.
    """
    title_links = []
    if soup:
        main_table = soup.find('table', id='maintable')
        if main_table:
            links = main_table.find_all('a', href=lambda href: href and 'App_mode=Display_Index' in href and 'Title_Request' in href)
            for link in links:
                title_text = link.get_text(strip=True)
                if title_text in required_titles:
                    from urllib.parse import urljoin
                    INDEX_URL = "http://www.leg.state.fl.us/Statutes/index.cfm?Mode=View%20Statutes&Submenu=1&Tab=statutes"
                    full_url = urljoin(INDEX_URL, link['href'])
                    title_links.append({'text': title_text, 'url': full_url})
    return title_links

def get_chapter_links(soup):
    """
    Extracts chapter links from a title page soup.
    Returns a list of dicts: {text, url}.
    """
    chapter_links = []
    if soup:
        BASE_URL = "http://www.leg.state.fl.us/Statutes/"
        links = soup.find_all('a', href=lambda href: href and 'App_mode=Display_Statute' in href and 'ContentsIndex.html' in href and 'StatuteYear=' in href)
        for link in links:
            from urllib.parse import urljoin
            full_url = urljoin(BASE_URL, link['href'])
            chapter_links.append({'text': link.get_text(strip=True), 'url': full_url})
    return chapter_links

def get_statute_links(soup):
    """
    Extracts statute links from a chapter page soup.
    Returns a list of dicts: {text, url}.
    """
    statute_links = []
    if soup:
        BASE_URL = "http://www.leg.state.fl.us/Statutes/"
        links = soup.find_all('a', href=lambda href: href and 'App_mode=Display_Statute' in href and 'Sections/' in href and '.html' in href)
        for link in links:
            from urllib.parse import urljoin
            full_url = urljoin(BASE_URL, link['href'])
            statute_links.append({'text': link.get_text(strip=True), 'url': full_url})
    return statute_links

def extract_statute_data(soup, url):
    """
    Parses a statute page soup and extracts statute number, title, text, and subsections.
    Returns a dict with keys: url, number, title, text, subsections.
    """
    statute_data = {'url': url, 'number': 'Not Found', 'title': 'Not Found', 'text': '', 'subsections': []}
    if soup:
        title_element = soup.find('h2')
        if title_element:
            title_text = title_element.get_text(strip=True)
            statute_data['title'] = title_text
            match = re.search(r'^\(?([0-9]+\.[0-9]+)\)?', title_text)
            if match:
                statute_data['number'] = match.group(1)
        main_content = soup.find('table', id='maintable')
        if main_content:
            text_elements = main_content.find_all(['p', 'div', 'span'])
            current_subsection = None
            full_text_parts = []
            for element in text_elements:
                text = element.get_text(strip=True)
                if not text or "Online Sunshine" in text or "Statutes & Constitution" in text or "Select Year:" in text:
                    continue
                subsection_match = re.match(r'^[\(\[]?([0-9]+|\w+)[\)\.,:]?\s+', text)
                if subsection_match:
                    if current_subsection:
                        statute_data['subsections'].append(current_subsection)
                    label = subsection_match.group(1)
                    subsection_text = text[subsection_match.end():].strip()
                    current_subsection = {'label': label, 'text': subsection_text}
                else:
                    if current_subsection:
                        current_subsection['text'] += "\n" + text
                    else:
                        full_text_parts.append(text)
            if current_subsection:
                statute_data['subsections'].append(current_subsection)
            statute_data['text'] = "\n".join(full_text_parts)
            if not statute_data['subsections']:
                statute_data['text'] = "\n".join([el.get_text(strip=True) for el in text_elements if el.get_text(strip=True) and "Online Sunshine" not in el.get_text(strip=True) and "Statutes & Constitution" not in el.get_text(strip=True) and "Select Year:" not in el.get_text(strip=True)])
    return statute_data

def scan_references(text):
    """
    Scans statute text for references to other statutes or chapters.
    Returns a list of reference dicts.
    """
    pattern = re.compile(r"s\.\s?([0-9]+\.[0-9]+)(?:[ \(]?(?:through|and|,|\s)+([0-9]+\.[0-9]+))?|ss\.\s?([0-9]+\.[0-9]+)(?:[ \(]?(?:through|and|,|\s)+([0-9]+\.[0-9]+))?|chapter\s?([0-9]+)", re.IGNORECASE)
    references = []
    for m in pattern.finditer(text):
        ref_text = m.group(0)
        context_snippet = text[max(0, m.start()-100):m.end()+100]
        if m.group(1):
            references.append({'type': 'section', 'section': m.group(1), 'context': context_snippet, 'raw_text': ref_text})
            if m.group(2):
                references.append({'type': 'section_range', 'start_section': m.group(1), 'end_section': m.group(2), 'context': context_snippet, 'raw_text': ref_text})
        elif m.group(3):
            references.append({'type': 'sections', 'section': m.group(3), 'context': context_snippet, 'raw_text': ref_text})
            if m.group(4):
                references.append({'type': 'sections_range', 'start_section': m.group(3), 'end_section': m.group(4), 'context': context_snippet, 'raw_text': ref_text})
        elif m.group(5):
            references.append({'type': 'chapter', 'chapter': m.group(5), 'context': context_snippet, 'raw_text': ref_text})
    return references

"""# Task
Create a scraping tool that will extract and parse the statutes from this website: "http://www.leg.state.fl.us/Statutes/index.cfm?Mode=View%20Statutes&Submenu=1&Tab=statutes&CFID=186340791&CFTOKEN=5a63c2a33464d9c-80C0FB7C-F34A-72F1-0AE1DD5D1A6EF6BB" and store the data in a sqlite database.

## Identify the structure of the website

### Subtask:
Analyze the HTML structure of the website to understand how the statutes are organized and how to navigate through them.

**Reasoning**:
Since I cannot directly interact with a web browser or its developer tools, I will simulate the process by fetching the HTML content of the provided URL using Python's `requests` library and then use `BeautifulSoup` to parse the HTML and analyze its structure. This will allow me to identify relevant HTML elements containing statute information or links, mimicking the steps outlined in the instructions.
"""

# Make required_titles a module-level variable for patching/testing
required_titles = [
    "TITLE I", "TITLE II", "TITLE III", "TITLE IV", "TITLE V", "TITLE VI",
    "TITLE VII", "TITLE VIII", "TITLE IX", "TITLE X", "TITLE XI", "TITLE XII", "TITLE XIII", "TITLE XIV", "TITLE XV", "TITLE XVI", "TITLE XVII", "TITLE XVIII", "TITLE XIX", "TITLE XX", "TITLE XXI", "TITLE XXII", "TITLE XXIII", "TITLE XXIV", "TITLE XXV", "TITLE XXVI", "TITLE XXVII", "TITLE XXVIII", "TITLE XXIX", "TITLE XXX", "TITLE XXXI", "TITLE XXXII", "TITLE XXXIII", "TITLE XXXIV", "TITLE XXXV", "TITLE XXXVI", "TITLE XXXVII", "TITLE XXXVIII", "TITLE XXXIX", "TITLE XL", "TITLE XLI", "TITLE XLII", "TITLE XLIII", "TITLE XLIV", "TITLE XLV", "TITLE XLVI", "TITLE XLVII", "TITLE XLVIII", "TITLE XLIX"
]

def main():
    config = configparser.RawConfigParser()
    config.read('config.ini')
    cfg = config['DEFAULT']
    INDEX_URL = cfg.get('index_url') or "http://www.leg.state.fl.us/Statutes/index.cfm?Mode=View%20Statutes&Submenu=1&Tab=statutes"
    db_file = cfg.get('db_file') or "statutes.db"
    rate_limit_seconds = float(cfg.get('rate_limit_seconds', 1) or 1)
    user_agent = cfg.get('user_agent') or "Palm Beach County Public Defender - assembling local statute database for public defense purposes. Contact: [your-email@pbcgov.org]"
    BASE_URL = "http://www.leg.state.fl.us/Statutes/"
    start_time = time.time()
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS Statutes (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        number TEXT,
        title TEXT,
        url TEXT,
        text TEXT,
        completed INTEGER DEFAULT 0
    )''')
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS Subsections (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        statute_id INTEGER,
        label TEXT,
        text TEXT,
        FOREIGN KEY (statute_id) REFERENCES Statutes (id)
    )''')
    conn.commit()
    result = fetch_html(INDEX_URL)
    if not result or result[0] is None:
        logger.error("Failed to fetch the index page.")
        conn.close()
        return
    initial_soup, _ = result
    title_links = get_title_links(initial_soup, required_titles)
    total_titles = len(title_links)
    logger.info(f"Found {total_titles} required Titles out of {len(required_titles)}.")
    # Statutes processing loop (refactored for resumability)
    for idx, title_link in enumerate(title_links, 1):
        title_text = title_link['text']
        title_url = title_link['url']
        logger.info(f"Processing {title_text} ({title_url})...")
        result = fetch_html(title_url)
        if not result or result[0] is None:
            logger.warning(f"Failed to fetch {title_url}")
            continue
        title_soup, _ = result
        chapter_links = get_chapter_links(title_soup)
        for chapter_link in chapter_links:
            chapter_url = chapter_link['url']
            result = fetch_html(chapter_url)
            if not result or result[0] is None:
                logger.warning(f"Failed to fetch {chapter_url}")
                continue
            chapter_soup, _ = result
            statute_links = get_statute_links(chapter_soup)
            for statute_link in statute_links:
                statute_url = statute_link['url']
                result = fetch_html(statute_url)
                if not result or result[0] is None:
                    logger.warning(f"Failed to fetch {statute_url}")
                    continue
                statute_soup, _ = result
                statute_data = extract_statute_data(statute_soup, statute_url)
                # Resumability: check if already completed
                cursor.execute('SELECT id, completed FROM Statutes WHERE number=? AND title=?', (statute_data['number'], statute_data['title']))
                row = cursor.fetchone()
                if row:
                    statute_id, completed = row
                    if completed == 1:
                        logger.info(f"Skipping already completed statute: {statute_data['number']} {statute_data['title']}")
                        continue
                    else:
                        logger.info(f"Resuming incomplete statute: {statute_data['number']} {statute_data['title']}")
                else:
                    cursor.execute('''
                    INSERT INTO Statutes (number, title, url, text, completed)
                    VALUES (?, ?, ?, ?, 0)''', (statute_data['number'], statute_data['title'], statute_data['url'], statute_data['text']))
                    statute_id = cursor.lastrowid
                # (Re-)insert subsections (delete old if resuming)
                cursor.execute('DELETE FROM Subsections WHERE statute_id=?', (statute_id,))
                for subsection in statute_data['subsections']:
                    cursor.execute('''
                    INSERT INTO Subsections (statute_id, label, text)
                    VALUES (?, ?, ?)''', (statute_id, subsection['label'], subsection['text']))
                # Mark as completed
                cursor.execute('UPDATE Statutes SET completed=1, text=? WHERE id=?', (statute_data['text'], statute_id))
                conn.commit()
                time.sleep(rate_limit_seconds)
        percent = (idx / total_titles) * 100
        elapsed = time.time() - start_time
        avg_time_per_title = elapsed / idx
        est_time_left = avg_time_per_title * (total_titles - idx)
        logger.info(f"Progress: {idx}/{total_titles} titles ({percent:.1f}%) complete. Estimated time left: {est_time_left/60:.1f} minutes.")
    logger.info("All statutes processed and stored in the database.")
    conn.close()

if __name__ == "__main__":
    main()

    """# References

    Throughout the development of this scraping tool, various online resources and documentation were referenced to implement the functionality and resolve any issues encountered. Some of the key references include:

    - [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): For guidance on using Beautiful Soup to parse HTML and XML documents.
    - [Requests Documentation](https://docs.python-requests.org/en/master/): For information on how to use the Requests library to make HTTP requests in Python.
    - [SQLite Documentation](https://www.sqlite.org/docs.html): For details on how to use SQLite and its Python bindings to manage the database.
    - [Python re module](https://docs.python.org/3/library/re.html): For understanding regular expressions in Python, used for pattern matching in statute text.
    - [Python urllib.parse module](https://docs.python.org/3/library/urllib.parse.html): For using URL parsing functions to handle and manipulate URLs.
    """

    """# Future Work

    There are several potential improvements and extensions that could be made to this scraping tool in the future:

    - **Error Handling and Resilience**: Improve the error handling and resilience of the tool, such as by implementing more robust retry logic, handling different types of HTTP errors, and validating the integrity of the scraped data.
    - **Dynamic Content Handling**: Enhance the tool to handle dynamic content loaded via JavaScript, potentially using a headless browser automation tool like Selenium.
    - **Data Model Refinement**: Refine the data model and database schema to capture additional details or relationships in the statute data, such as annotations, historical versions, or related statutes.
    - **User Interface**: Develop a user interface to allow users to configure the scraping parameters, view the progress, and manage the scraped data more easily.
    - **Documentation and Testing**: Improve the documentation and testing of the tool to ensure its reliability and ease of use.
    """